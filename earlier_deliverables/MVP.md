**MVP: Predicting Emotions an Artwork evokes** 

I tried 3 different model-series on 3 different datasets that were supplied by the researchers: 
1. (T50) Initially I built my model with the dataset in which an emotion was marked as 'evoked' if at least 50% of the participants said so. With this model, I tried: (1)basic model with 1 Dense layer (~20% acc), (2)model with 4 convolution layers with dropout layers added (~22-23% acc), (3, 4)2 different models with 4CNN & data aug(23-24%), (5)mobile with transfer learning from MobileNet/Imagenet combo (~24-25%), (6)model with Mobile Net + data augmentation (25%), (7, 8)2 models with MobileNet+ data aug combo adding some dropout + regularization (down to ~22%, but also adaptod EarlyStopping here a bit), and (9)finally another model with VGG16 as opposed to MobileNet (22%). 
2. (T30) Then I went back and started using another dataset from the authors in which they noted an emotion as 'evoked' if at least 30% of the participants marked it so. I went thru the same models listed above, but had slight improvements (since the emotions had more 'present' as opposed to 'absent' with this dataset). With this one I started with 20 and inched towards 30%. 
3. (Single Class) At the very end, I also decided to work with yet different dataset in which emotions are listed as 'percentages' as opposed to being one-hot-encoded. Which means, I just took the top percentage, as *the* emotion evoked. So then the model turned into a classic classifier with 10 classes as opposed to multi label classifier (even tho there are 20 classes, the ones that show up frequent enough are 10 of these, and I expected an emotion to turn up at least 50 times to call it 'evoked'). With this final model, my accuracy hovered around 27.5% (only ran the first and the last models noted above). 

Now I'm going to the basics and will try binary classification with 2 emotions. 
